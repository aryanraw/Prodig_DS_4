# Prodigy Data Science Internship Task 4

This repository contains the code for Task 4 of the Prodigy Data Science Internship. Task 4 involves preprocessing and exploratory data analysis (EDA) of Twitter training and validation datasets.

## Task Description
"Analyze and visualize sentiment patterns in social media data to understand public opinion and attitudes towards specific topics or brands"

## Dataset
The datasets used for this task are:
- `twitter_training.csv`: Training dataset containing tweets with sentiment labels and associated brands.
- `twitter_validation.csv`: Validation dataset containing tweets with sentiment labels and associated brands.

## Code Overview
- **Importing Libraries:** Importing necessary libraries including Pandas, NumPy, Matplotlib, Seaborn, and NLTK.
- **Loading Datasets:** Loading the Twitter training and validation datasets.
- **Data Preprocessing:** Handling missing values and dropping unnecessary columns.
- **Exploratory Data Analysis (EDA):** Exploring the distribution of sentiment, brands, and other variables in the datasets using visualizations.
- **Label Encoding:** Encoding categorical variables using label encoding.
- **Analyzing Data:** Analyzing the distribution of sentiment and brands in the datasets.

## Repository Structure
- `twitter_training.csv`: Training dataset containing tweets with sentiment labels and associated brands.
- `twitter_validation.csv`: Validation dataset containing tweets with sentiment labels and associated brands.
- `task_4_code.py`: Python script containing the code for Task 4.
- `README.md`: Readme file providing an overview of the repository.

## Usage
1. Clone the repository to your local machine.
2. Install the required dependencies (`pandas`, `numpy`, `matplotlib`, `seaborn`, `nltk`).
3. Run the `task_4_code.py` script to execute the code.

## Dependencies
- Pandas
- NumPy
- Matplotlib
- Seaborn
- NLTK

## Contributors
- Aryan Singh Rawat ([aryanraw](https://github.com/aryanraw))
